import pandas as pd
import json
import pathlib
import subprocess
import shutil
import cv2
import os
from tqdm import tqdm

# ================= é…ç½®åŒº =================
# 1. ä½ çš„æ•°æ®é›†æ ¹ç›®å½•
DATASET_PATH = pathlib.Path("/home/jr/PI/data/mkrobot_cube_dataset")
# 2. å¤–éƒ¨å¤‡ä»½ç›®å½•ï¼ˆå¿…é¡»åœ¨ DATASET_PATH ä¹‹å¤–ï¼Œé˜²æ­¢è¢« LeRobot é€’å½’æ‰«æï¼‰
EXTERNAL_BACKUP_PATH = pathlib.Path("/home/jr/PI/data_backups/mkrobot_raw_data")
# 3. ä»»åŠ¡æŒ‡ä»¤
CORRECT_TASK = "pick up the small cube and place it in the box"
FPS = 30
# =========================================

def run_cmd(cmd):
    return subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

def get_video_frame_count(video_path):
    cap = cv2.VideoCapture(str(video_path))
    count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.release()
    return count

def main():
    print(f"ğŸš€ å¼€å§‹æ ‡å‡†åŒ–æ•°æ®é›†ï¼ˆå«æ·±åº¦ç‰©ç†æ¸…ç†ï¼‰: {DATASET_PATH}")
    
    data_dir = DATASET_PATH / "data" / "chunk-000"
    meta_dir = DATASET_PATH / "meta"
    video_root = DATASET_PATH / "videos"
    
    # --- STEP 0: ç‰©ç†éš”ç¦»ä¸æ·±åº¦æ¸…ç† ---
    print("Step 0: æ­£åœ¨æ¬ç¦»åŸå§‹æ–‡ä»¶å¹¶æ¸…ç†å†—ä½™å…ƒæ•°æ®...")
    EXTERNAL_BACKUP_PATH.mkdir(parents=True, exist_ok=True)

    # 1. æ¬ç¦»åŸå§‹ Parquet èšåˆæ–‡ä»¶åˆ°å¤–éƒ¨ï¼ˆå¦‚æœè¿˜åœ¨ data ç›®å½•ä¸‹ï¼‰
    for f in data_dir.glob("file-*.parquet"):
        print(f"ğŸ“¦ æ¬è¿åŸå§‹æ•°æ®: {f.name}")
        shutil.move(str(f), str(EXTERNAL_BACKUP_PATH / f.name))
        
    # 2. æ¬ç¦»ä¹‹å‰çš„ raw_backup ç›®å½•åˆ°å¤–éƒ¨ï¼ˆé˜²æ­¢é€’å½’æ‰«æå¯¼è‡´æ•°æ®ç¿»å€ï¼‰
    old_backup = DATASET_PATH / "data" / "raw_backup"
    if old_backup.exists():
        staging_backup = EXTERNAL_BACKUP_PATH / "raw_backup"
        if not staging_backup.exists():
            print(f"ğŸ“¦ æ¬è¿å¤‡ä»½ç›®å½•: {old_backup.name}")
            shutil.move(str(old_backup), str(staging_backup))
        else:
            print(f"ğŸ—‘ï¸ åˆ é™¤é‡å¤å¤‡ä»½ç›®å½•: {old_backup}")
            shutil.rmtree(old_backup)

    # 3. åˆ é™¤å¯¼è‡´å¹²æ‰°çš„æ—§å…ƒæ•°æ®æ–‡ä»¶
    files_to_delete = [
        meta_dir / "episodes",       # è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹
        meta_dir / "tasks.parquet",  # å†—ä½™æ–‡ä»¶
        meta_dir / "stats.json"      # å†—ä½™æ–‡ä»¶
    ]
    for path in files_to_delete:
        if path.exists():
            if path.is_dir():
                print(f"ğŸ—‘ï¸ åˆ é™¤ç›®å½•: {path.name}")
                shutil.rmtree(path)
            else:
                print(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {path.name}")
                path.unlink()

    # --- STEP 1: é‡æ–°åŠ è½½å¹¶æ ‡å‡†åŒ–æ•°æ® ---
    print("\nStep 1: æ‹†åˆ†å¹¶æ ‡å‡†åŒ– Parquet æ•°æ® (Index & Timestamp)...")
    # ä»å¤–éƒ¨å¤‡ä»½è¯»å–åŸå§‹æ•°æ®
    raw_files = sorted(EXTERNAL_BACKUP_PATH.glob("file-*.parquet"))
    if not raw_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {EXTERNAL_BACKUP_PATH} ä¸‹æ²¡æ‰¾åˆ°åŸå§‹æ–‡ä»¶ï¼")
        return
        
    full_df = pd.concat([pd.read_parquet(f) for f in raw_files], ignore_index=True)
    
    # æ¸…ç† data ç›®å½•ä¸‹æ—§çš„ episode æ–‡ä»¶
    for f in data_dir.glob("episode_*.parquet"): f.unlink()

    episodes = sorted(full_df["episode_index"].unique())
    time_step = 1.0 / FPS
    ep_info_list = []
    
    for ep_idx in tqdm(episodes, desc="å¤„ç† Parquet"):
        ep_df = full_df[full_df["episode_index"] == ep_idx].copy().sort_values("index")
        orig_start_idx = ep_df["index"].min()
        num_frames = len(ep_df)
        
        ep_df["index"] = range(num_frames)
        ep_df["timestamp"] = [float(i * time_step) for i in range(num_frames)]
        
        out_path = data_dir / f"episode_{int(ep_idx):06d}.parquet"
        ep_df.to_parquet(out_path, index=False)
        ep_info_list.append({"index": int(ep_idx), "length": num_frames, "orig_start": orig_start_idx})

    # --- STEP 2: ç‰©ç†è£å‰ªè§†é¢‘å¹¶é‡ç½® PTS ---
    print("\nStep 2: å¸§å‡†ç¡®è§†é¢‘è£å‰ª (é‡ç½® PTS æ—¶é—´æˆ³)...")
    for cam in ["observation.images.top", "observation.images.wrist"]:
        cam_dir = video_root / cam / "chunk-000"
        
        # æŸ¥æ‰¾åŸå§‹è§†é¢‘ï¼ˆå¯èƒ½åœ¨ cam_dir ä¹Ÿå¯ä»¥åœ¨å¤–éƒ¨å¤‡ä»½ï¼‰
        raw_videos = sorted(cam_dir.glob("file-*.mp4"))
        
        video_map = []
        offset = 0
        for v in raw_videos:
            cnt = get_video_frame_count(v)
            video_map.append({"path": v, "start": offset, "end": offset + cnt - 1})
            offset += cnt

        for ep in tqdm(ep_info_list, desc=f"è£å‰ª {cam}"):
            source = next((v for v in video_map if v["start"] <= ep["orig_start"] <= v["end"]), None)
            if not source: continue
            
            local_start = ep["orig_start"] - source["start"]
            local_end = local_start + ep["length"] - 1
            out_video = cam_dir / f"episode_{ep['index']:06d}.mp4"
            
            cmd = [
                'ffmpeg', '-y', '-i', str(source["path"]),
                '-vf', f"select='between(n,{local_start},{local_end})',setpts=PTS-STARTPTS",
                '-vsync', '0', '-c:v', 'libx264', '-preset', 'veryfast', '-crf', '18', '-pix_fmt', 'yuv420p',
                str(out_video)
            ]
            run_cmd(cmd)
            
        # è£å‰ªå®Œæˆåï¼Œå°†åŸå§‹è§†é¢‘å¤§æ–‡ä»¶æ¬ç¦»ï¼Œé˜²æ­¢å¹²æ‰°ç´¢å¼•
        for v in raw_videos:
            dest = EXTERNAL_BACKUP_PATH / v.name
            if not dest.exists():
                print(f"ğŸ“¦ æ¬ç¦»è§†é¢‘åŸä»¶: {v.name}")
                shutil.move(str(v), str(dest))
            else:
                v.unlink()

    # --- STEP 3: è¡¥å…¨å…ƒæ•°æ® ---
    print("\nStep 3: å¼ºåˆ¶åˆ·æ–°å…ƒæ•°æ® (è¡¥å…¨ stats/length/task)...")
    
    with open(meta_dir / "tasks.jsonl", "w", encoding="utf-8") as f:
        f.write(json.dumps({"task_index": 0, "task": CORRECT_TASK}) + "\n")
    
    with open(meta_dir / "episodes.jsonl", "w", encoding="utf-8") as f:
        for ep in ep_info_list:
            f.write(json.dumps({"episode_index": ep["index"], "tasks": [CORRECT_TASK], "length": ep["length"]}) + "\n")
            
    with open(meta_dir / "episodes_stats.jsonl", "w", encoding="utf-8") as f:
        for ep in ep_info_list:
            f.write(json.dumps({"episode_index": ep["index"], "stats": {}}) + "\n")

    info_path = meta_dir / "info.json"
    with open(info_path, "r") as f:
        info = json.load(f)
    info.update({
        "codebase_version": "v2.1",
        "total_episodes": len(episodes),
        "data_path": "data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet",
        "video_path": "videos/{video_key}/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.mp4"
    })
    with open(info_path, "w") as f:
        json.dump(info, f, indent=4)

    # --- STEP 4: æ¸…ç†ç¼“å­˜ ---
    cache_dir = pathlib.Path.home() / ".cache/huggingface/datasets"
    if cache_dir.exists():
        print(f"\nğŸ—‘ï¸ æ¸…ç†æ•°æ®é›†ç¼“å­˜: {cache_dir}")
        shutil.rmtree(cache_dir)
    
    print("\nâœ¨ æ­å–œï¼ä¸€é”®æ ‡å‡†åŒ–ä¸æ·±åº¦ç‰©ç†æ¸…ç†å·²å®Œæˆã€‚")
    print(f"åŸå§‹æ–‡ä»¶å·²å®‰å…¨æ¬è¿è‡³: {EXTERNAL_BACKUP_PATH}")

if __name__ == "__main__":
    main()